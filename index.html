
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Python Code Library</title>
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      body {
        font-family: "Segoe UI", Tahoma, Geneva, Verdana, sans-serif;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        min-height: 100vh;
      }

      .container {
        max-width: 1400px;
        margin: 0 auto;
        background: white;
        min-height: 100vh;
      }

      .header {
        background: linear-gradient(135deg, #2d3748 0%, #1a202c 100%);
        color: white;
        padding: 30px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
      }

      .header h1 {
        font-size: 2em;
        margin-bottom: 10px;
      }

      .header p {
        opacity: 0.8;
      }

      .content-wrapper {
        display: flex;
        min-height: calc(100vh - 120px);
      }

      .sidebar {
        width: 280px;
        background: #f7fafc;
        border-right: 2px solid #e2e8f0;
        padding: 20px;
      }

      .nav-title {
        font-size: 0.9em;
        color: #718096;
        text-transform: uppercase;
        letter-spacing: 1px;
        margin-bottom: 15px;
        font-weight: 600;
      }

      .nav-link {
        display: block;
        padding: 12px 16px;
        color: #2d3748;
        text-decoration: none;
        border-radius: 6px;
        margin-bottom: 8px;
        transition: all 0.2s ease;
        font-weight: 500;
        border: 1px solid black;
      }

      .nav-link:hover {
        background: #e2e8f0;
        transform: translateX(5px);
      }

      .nav-link.active {
        background: #667eea;
        color: white;
      }

      .main-content {
        flex: 1;
        padding: 40px;
        overflow-y: auto;
      }

      .page {
        display: none;
      }

      .page.active {
        display: block;
        animation: fadeIn 0.3s ease;
      }

      @keyframes fadeIn {
        from {
          opacity: 0;
          transform: translateY(10px);
        }
        to {
          opacity: 1;
          transform: translateY(0);
        }
      }

      .page-title {
        font-size: 2em;
        color: #2d3748;
        margin-bottom: 30px;
        padding-bottom: 15px;
        border-bottom: 3px solid #667eea;
      }

      .text-section {
        color: #2d3748;
        margin-bottom: 25px;
        font-size: 1.05em;
        line-height: 1.7;
        background: #f7fafc;
        padding: 20px;
        border-radius: 8px;
        border-left: 4px solid #667eea;
      }

      .text-section h3 {
        margin-bottom: 12px;
        color: #667eea;
      }

      .text-section p {
        margin-bottom: 10px;
      }

      .text-section ul {
        margin-left: 25px;
        margin-top: 10px;
      }

      .text-section li {
        margin-bottom: 8px;
      }

      .code-block {
        background: #1e1e1e;
        border-radius: 8px;
        padding: 25px;
        overflow-x: auto;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        margin-bottom: 25px;
      }

      pre {
        margin: 0;
        font-family: "Courier New", monospace;
        font-size: 14px;
        line-height: 1.8;
        color: #d4d4d4;
      }

      @media (max-width: 768px) {
        .content-wrapper {
          flex-direction: column;
        }

        .sidebar {
          width: 100%;
          border-right: none;
          border-bottom: 2px solid #e2e8f0;
        }

        .main-content {
          padding: 20px;
        }
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="header">
        <h1>ğŸ“š Python Code Library</h1>
        <p>Browse and explore Python code examples</p>
      </div>

      <div class="content-wrapper">
        <nav class="sidebar">
          <div class="nav-title">Code Examples</div>
          <a href="#" class="nav-link active" data-page="0"
            >áƒ¬áƒ áƒ¤áƒ˜áƒ•áƒ˜ áƒ áƒ”áƒ’áƒ”áƒ áƒ¡áƒ˜áƒ, áƒáƒ áƒáƒ’áƒœáƒáƒ–áƒ˜áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒáƒ›áƒáƒªáƒáƒœáƒ (áƒ¨áƒ”áƒ¡áƒáƒ‘áƒáƒ›áƒ˜áƒ¡áƒ˜ áƒ›áƒ”áƒ¢áƒ áƒ˜áƒ™áƒ”áƒ‘áƒ˜áƒ¡
            áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ); áƒ™áƒšáƒáƒ¡áƒ˜áƒ¤áƒ˜áƒ™áƒáƒªáƒ˜áƒ˜áƒ¡ áƒáƒ›áƒáƒªáƒáƒœáƒ”áƒ‘áƒ˜ (áƒ‘áƒ˜áƒœáƒáƒ áƒ£áƒšáƒ˜, áƒ›áƒ áƒáƒ•áƒáƒšáƒ™áƒšáƒáƒ¡áƒ˜áƒáƒœáƒ˜)
            áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜ áƒ“áƒ áƒ•áƒ˜áƒ–áƒ£áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ
            <br />
            <br />
            áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒ‘áƒáƒ–áƒ˜áƒ¡ áƒ‘áƒáƒšáƒáƒœáƒ¡áƒ˜áƒ áƒ”áƒ‘áƒ£áƒšáƒáƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ (countplot)
            áƒ›áƒáƒ®áƒáƒ¡áƒ˜áƒáƒ—áƒ”áƒ‘áƒšáƒ”áƒ‘áƒ¡ áƒ¨áƒáƒ áƒ˜áƒ¡ áƒ™áƒáƒ áƒ”áƒšáƒáƒªáƒ˜áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ—áƒ•áƒšáƒ áƒ™áƒáƒ áƒ”áƒšáƒáƒªáƒ˜áƒ˜áƒ¡ áƒ›áƒáƒ¢áƒ áƒ˜áƒªáƒ˜áƒ¡
            áƒ•áƒ˜áƒ–áƒ£áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ (heatmap)</a
          >
          <a href="#" class="nav-link" data-page="1">
            áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒ¬áƒ˜áƒœáƒáƒ¡áƒ¬áƒáƒ áƒ˜ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ (Preprocessing)
            <br />
            <br />
            áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒ“áƒáƒ§áƒáƒ¤áƒ áƒ¡áƒáƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒ áƒ“áƒ áƒ¡áƒáƒ¢áƒ”áƒ¡áƒ¢áƒ áƒœáƒáƒ™áƒ áƒ”áƒ‘áƒáƒ“ áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ
            áƒœáƒáƒ áƒ›áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ áƒ“áƒ áƒ¡áƒ¢áƒáƒœáƒ“áƒáƒ áƒ¢áƒ˜áƒ–áƒáƒªáƒ˜áƒ (StandardScaler, MinMaxScaler)
            preprocessing-áƒ˜áƒ¡ áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒ áƒ›áƒáƒœáƒ¥áƒáƒœáƒ£áƒ  áƒ¡áƒ¬áƒáƒ•áƒšáƒ”áƒ‘áƒáƒ¨áƒ˜
          </a>
          <a href="#" class="nav-link" data-page="2">
            áƒ™áƒšáƒáƒ¡áƒ˜áƒ¤áƒ˜áƒ™áƒáƒªáƒ˜áƒ˜áƒ¡ áƒáƒšáƒ’áƒáƒ áƒ˜áƒ—áƒ›áƒ”áƒ‘áƒ˜
            <br />
            <br />
            Support Vector Classifier (SVC) K-Nearest Neighbors (KNN) Gaussian
            Naive Bayes Decision Tree Random Forest Classifier
          </a>
          <a href="#" class="nav-link" data-page="3">
            áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¨áƒ”áƒ¤áƒáƒ¡áƒ”áƒ‘áƒ
            <br />
            <br />
            Confusion Matrix áƒ“áƒ áƒ›áƒ˜áƒ¡áƒ˜ áƒ™áƒáƒ›áƒáƒáƒœáƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜ (True Positive, False
            Positive, True Negative, False Negative) áƒ¨áƒ”áƒ¤áƒáƒ¡áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒ”áƒ¢áƒ áƒ˜áƒ™áƒ”áƒ‘áƒ˜:
            Accuracy, Precision, Recall, F1-score áƒ›áƒ”áƒ¢áƒ áƒ˜áƒ™áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ
            áƒ‘áƒáƒšáƒáƒœáƒ¡áƒ˜áƒ áƒ”áƒ‘áƒ£áƒš áƒ“áƒ áƒáƒ áƒáƒ‘áƒáƒšáƒáƒœáƒ¡áƒ˜áƒ áƒ”áƒ‘áƒ£áƒš áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒ‘áƒáƒ–áƒ”áƒ‘áƒ¨áƒ˜.
          </a>
          <a href="#" class="nav-link" data-page="4">
            áƒ°áƒ˜áƒáƒ”áƒ áƒáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒáƒáƒ¢áƒ˜áƒ›áƒ˜áƒ–áƒáƒªáƒ˜áƒ
            <br />
            <br />GridSearchCV-áƒ˜áƒ¡ áƒ“áƒáƒœáƒ˜áƒ¨áƒœáƒ£áƒšáƒ”áƒ‘áƒ áƒ“áƒ áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒ˜áƒ¡ áƒáƒ áƒ˜áƒœáƒªáƒ˜áƒáƒ˜
            áƒ°áƒ˜áƒáƒ”áƒ áƒáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ˜áƒ¡ áƒªáƒœáƒ”áƒ‘áƒ GridSearchCV-áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ
            Cross-Validation-áƒ—áƒáƒœ áƒ”áƒ áƒ—áƒáƒ“
          </a>
          <a href="#" class="nav-link" data-page="5">
            Cross-Validation
            <br />
            <br />
            Cross-Validation-áƒ˜áƒ¡ áƒáƒ áƒ¡áƒ˜ áƒ“áƒ áƒ“áƒáƒœáƒ˜áƒ¨áƒœáƒ£áƒšáƒ”áƒ‘áƒ K-Fold Cross-Validation-áƒ˜áƒ¡
            áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒ˜áƒ¡ áƒáƒ áƒ˜áƒœáƒªáƒ˜áƒáƒ˜ áƒ£áƒáƒ˜áƒ áƒáƒ¢áƒ”áƒ¡áƒáƒ‘áƒ”áƒ‘áƒ˜ áƒ”áƒ áƒ—áƒ¯áƒ”áƒ áƒáƒ“ train/test áƒ’áƒáƒ§áƒáƒ¤áƒáƒ¡áƒ—áƒáƒœ
            áƒ¨áƒ”áƒ“áƒáƒ áƒ”áƒ‘áƒ˜áƒ—
          </a>
          <a href="#" class="nav-link" data-page="6">
            Overfitting áƒ“áƒ Underfitting
            <br />
            <br />
            áƒªáƒœáƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒœáƒ›áƒáƒ áƒ¢áƒ”áƒ‘áƒ Overfitting-áƒ˜áƒ¡ áƒáƒ›áƒáƒªáƒœáƒáƒ‘áƒ˜áƒ¡ áƒœáƒ˜áƒ¨áƒœáƒ”áƒ‘áƒ˜ Overfitting-áƒ˜áƒ¡
            áƒ—áƒáƒ•áƒ˜áƒ“áƒáƒœ áƒáƒªáƒ˜áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒ”áƒ—áƒáƒ“áƒ”áƒ‘áƒ˜
          </a>
          <a href="#" class="nav-link" data-page="7"
            >áƒ™áƒšáƒáƒ¡áƒ¢áƒ”áƒ áƒ˜áƒ–áƒáƒªáƒ˜áƒ <br />
            <br />
            K-means áƒ™áƒšáƒáƒ¡áƒ¢áƒ”áƒ áƒ˜áƒ–áƒáƒªáƒ˜áƒ˜áƒ¡ áƒáƒšáƒ’áƒáƒ áƒ˜áƒ—áƒ›áƒ˜ áƒáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ˜ K K-means-áƒ˜áƒ¡
            áƒ¨áƒ”áƒ–áƒ¦áƒ£áƒ“áƒ•áƒ”áƒ‘áƒ˜</a
          >
          <a href="#" class="nav-link" data-page="8"
            >áƒ’áƒáƒœáƒ–áƒáƒ›áƒ˜áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒªáƒ˜áƒ áƒ”áƒ‘áƒ <br />
            <br />
            PCA (Principal Component Analysis) áƒ›áƒ—áƒáƒ•áƒáƒ áƒ˜ áƒ™áƒáƒ›áƒáƒáƒœáƒ”áƒœáƒ¢áƒ˜áƒ¡ áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒ
            áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒ›áƒáƒ¡áƒ¨áƒ¢áƒáƒ‘áƒ˜áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒáƒ£áƒªáƒ˜áƒšáƒ”áƒ‘áƒšáƒáƒ‘áƒ PCA-áƒ›áƒ“áƒ”</a
          >
        </nav>

        <main class="main-content" id="mainContent">
          <!-- Pages will be dynamically loaded here -->
        </main>
      </div>
    </div>

    <script>
      const sections = [
        {
          title: "Section 1 Title",
          content: [
            {
              type: "text",
              content: `<h3>áƒ¬áƒ áƒ¤áƒ˜áƒ•áƒ˜ áƒ áƒ”áƒ’áƒ áƒ”áƒ¡áƒ˜áƒ â€“ áƒáƒ áƒáƒ’áƒœáƒáƒ–áƒ˜áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒáƒ›áƒáƒªáƒáƒœáƒ (+ áƒ›áƒ”áƒ¢áƒ áƒ˜áƒ™áƒ”áƒ‘áƒ˜)</h3>`,
            },
            {
              type: "code",
              content: `# import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# sample data
df = pd.DataFrame({
    "size": [50, 60, 70, 80, 90],
    "rooms": [1, 2, 2, 3, 3],
    "price": [100, 120, 150, 180, 210]
})

# features / target
X = df[["size", "rooms"]]
y = df["price"]

# split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# model
model = LinearRegression()
model.fit(X_train, y_train)

# prediction
y_pred = model.predict(X_test)

# metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

mae, mse, r2
`,
            },
            {
              type: "text",
              content: `<h3>regression â†’ MAE, MSE, RÂ²

target is numeric</h3>`,
            },
            {
              type: "text",
              content: `<h3>áƒ™áƒšáƒáƒ¡áƒ˜áƒ¤áƒ˜áƒ™áƒáƒªáƒ˜áƒ˜áƒ¡ áƒáƒ›áƒáƒªáƒáƒœáƒ”áƒ‘áƒ˜</h3>
              <p>2.1 áƒ‘áƒ˜áƒœáƒáƒ áƒ£áƒšáƒ˜ áƒ™áƒšáƒáƒ¡áƒ˜áƒ¤áƒ˜áƒ™áƒáƒªáƒ˜áƒ (Binary)</p>
              <p>Example: pass / fail prediction</p>`,
            },

            {
              type: "code",
              content: `from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score

# data
df = pd.DataFrame({
    "study_hours": [2, 4, 6, 8, 10],
    "pass": [0, 0, 1, 1, 1]
})

X = df[["study_hours"]]
y = df["pass"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

# evaluation
cm = confusion_matrix(y_test, y_pred)
acc = accuracy_score(y_test, y_pred)

cm, acc
`,
            },
            {
              type: "text",
              content: `<h3>áƒ›áƒ áƒáƒ•áƒáƒšáƒ™áƒšáƒáƒ¡áƒ˜áƒáƒœáƒ˜ áƒ™áƒšáƒáƒ¡áƒ˜áƒ¤áƒ˜áƒ™áƒáƒªáƒ˜áƒ (Multiclass)</h3>
              <p>Example: wine class prediction</p>`,
            },
            {
              type: "code",
              content: `from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# load dataset
data = load_wine()
X = data.data
y = data.target   # 3 classes

# split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# model
model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
accuracy
`,
            },
            {
              type: "text",
              content: `
              <p>Multiclass â†’ 3+ classes, same workflow as binary</p>
              `,
            },

            {
              type: "text",
              content: `
              <h3>áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜ áƒ“áƒ áƒ•áƒ˜áƒ–áƒ£áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ (EDA)/h3>
              `,
            },

            {
              type: "code",
              content: `
              import seaborn as sns
import matplotlib.pyplot as plt

df = sns.load_dataset("iris")

              
              `,
            },
            {
              type: "text",
              content: `
              <h4>áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜/h4>
              `,
            },
            {
              type: "code",
              content: `
              df.head()
df.info()
df.describe()
              `,
            },
            {
              type: "text",
              content: `
              <h3>áƒ™áƒšáƒáƒ¡áƒ”áƒ‘áƒ˜áƒ¡ áƒ‘áƒáƒšáƒáƒœáƒ¡áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ (countplot)/h3>
              `,
            },
            {
              type: "code",
              content: `
              sns.countplot(x="species", data=df)
plt.show()
`,
            },
            {
              type: "text",
              content: `
              <h3>3.3 áƒ’áƒáƒœáƒáƒ¬áƒ˜áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ•áƒ˜áƒ–áƒ£áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ (Histogram)</h3>`,
            },

            {
              type: "code",
              content: `
              sns.histplot(df["sepal_length"], kde=True)
plt.show()
              `,
            },

            {
              type: "text",
              content: `
              <h3>áƒ›áƒáƒ®áƒáƒ¡áƒ˜áƒáƒ—áƒ”áƒ‘áƒšáƒ”áƒ‘áƒ¡ áƒ¨áƒáƒ áƒ˜áƒ¡ áƒ™áƒáƒ•áƒ¨áƒ˜áƒ áƒ˜ (Scatter)</h3>
              `,
            },

            {
              type: "code",
              content: `
              sns.scatterplot(
    x="sepal_length",
    y="petal_length",
    hue="species",
    data=df
)
plt.show()
`,
            },
            {
              type: "text",
              content: `
              <h3>áƒ™áƒáƒ áƒ”áƒšáƒáƒªáƒ˜áƒ + Heatmap</h3>
              `,
            },

            {
              type: "code",
              content: `
              corr = df.drop("species", axis=1).corr()

sns.heatmap(corr, annot=True, cmap="coolwarm")
plt.show()
              `,
            },
          ],
        },
        {
          title: "Section 3 Title",
          content: [
            {
              type: "text",
              content: `<h3>áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒ¬áƒ˜áƒœáƒáƒ¡áƒ¬áƒáƒ áƒ˜ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ (Preprocessing)</h3>
              <p>Example dataset:</p>`,
            },

            {
              type: "code",
              content: `import pandas as pd
import numpy as np

df = pd.DataFrame({
    "age": [20, 25, np.nan, 40, 35],
    "salary": [3000, 4000, 5000, np.nan, 6000],
    "score": [50, 60, 70, 80, 90],
    "target": [0, 1, 1, 0, 1]
})`,
            },

            {
              type: "text",
              content: `<p>Missing values-áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ</p>`,
            },

            {
              type: "code",
              content: `df.isnull().sum()`,
            },
            {
              type: "text",
              content: `<p>Replace with mean</p>`,
            },

            {
              type: "code",
              content: `df["age"].fillna(df["age"].mean(), inplace=True)
df["salary"].fillna(df["salary"].mean(), inplace=True)`,
            },
            {
              type: "text",
              content: `<h3>1.2 Feature / Target áƒ’áƒáƒ›áƒáƒ§áƒáƒ¤</h3>`,
            },

            {
              type: "code",
              content: `X = df.drop("target", axis=1)
y = df["target"]`,
            },

            {
              type: "text",
              content: `<h3>áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒ“áƒáƒ§áƒáƒ¤áƒ áƒ¡áƒáƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒ áƒ“áƒ áƒ¡áƒáƒ¢áƒ”áƒ¡áƒ¢áƒ áƒœáƒáƒ™áƒ áƒ”áƒ‘áƒáƒ“</h3>`,
            },

            {
              type: "code",
              content: `from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42
)`,
            },

            {
              type: "text",
              content: `<h3>áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒœáƒáƒ áƒ›áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ áƒ“áƒ áƒ¡áƒ¢áƒáƒœáƒ“áƒáƒ áƒ¢áƒ˜áƒ–áƒáƒªáƒ˜áƒ</h3>
              <p>StandardScaler (áƒ¡áƒ¢áƒáƒœáƒ“áƒáƒ áƒ¢áƒ˜áƒ–áƒáƒªáƒ˜áƒ)</p>`,
            },

            {
              type: "code",
              content: `from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


ğŸ“Œ Result:

mean = 0

std = 1

ğŸ“Œ Used for:
SVM, KNN, Linear Regression, PCA
`,
            },

            {
              type: "text",
              content: `<h3>MinMaxScaler (áƒœáƒáƒ áƒ›áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ)</h3>
              `,
            },

            {
              type: "code",
              content: `from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)



ğŸ“Œ Result:

values scaled to [0, 1]

ğŸ“Œ Used for:
distance-based algorithms (KNN, K-Means)
              `,
            },

            {
              type: "text",
              content: `<h3>preprocessing-áƒ˜áƒ¡ áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒ áƒ›áƒáƒœáƒ¥áƒáƒœáƒ£áƒ  áƒ¡áƒ¬áƒáƒ•áƒšáƒ”áƒ‘áƒáƒ¨áƒ˜</h3>
              `,
            },

            {
              type: "code",
              content: `
#Example: KNN without vs with scaling
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

#Without scaling
model = KNeighborsClassifier(n_neighbors=3)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy_score(y_test, y_pred)

# With scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = KNeighborsClassifier(n_neighbors=3)
model.fit(X_train_scaled, y_train)

y_pred = model.predict(X_test_scaled)
accuracy_score(y_test, y_pred)


# ğŸ“Œ Accuracy usually improves after preprocessing
              `,
            },
          ],
        },
        {
          title: "Section 4 Title",
          content: [
            {
              type: "text",
              content: `<h3>Common setup (USED FOR ALL MODELS)</h3>
              `,
            },
            {
              type: "code",
              content: `from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix

# load data
data = load_iris()
X = data.data
y = data.target   # multiclass (3 classes)

# split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# scaling (needed for some models)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Scaling helps SVC & KNN
`,
            },
            {
              type: "text",
              content: `<h3>Support Vector Classifier (SVC)</h3>
              `,
            },

            {
              type: "code",
              content: `from sklearn.svm import SVC

model = SVC(kernel="rbf", C=1.0)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

accuracy, cm

              `,
            },

            {
              type: "text",
              content: `<h3>K-Nearest Neighbors (KNN)</h3>
              `,
            },

            {
              type: "code",
              content: `from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

accuracy, cm
`,
            },

            {
              type: "text",
              content: `<h3>Gaussian Naive Bayes</h3>
              `,
            },

            {
              type: "code",
              content: `from sklearn.naive_bayes import GaussianNB

model = GaussianNB()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

accuracy, cm

              `,
            },

            {
              type: "text",
              content: `<h3>Decision Tree Classifier</h3>
              `,
            },

            {
              type: "code",
              content: `from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(
    criterion="gini",
    max_depth=None,
    random_state=42
)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

accuracy, cm

              `,
            },

            {
              type: "text",
              content: `<h3>Random Forest Classifier</h3>
              `,
            },
            {
              type: "code",
              content: `from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    n_estimators=100,
    random_state=42
)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

accuracy, cm

              `,
            },
            {
              type: "text",
              content: `<p>Model	Needs scaling	Overfitting risk	Fast
SVC	âœ… Yes	Medium	âŒ
KNN	âœ… Yes	Medium	âŒ
Naive Bayes	âŒ No	Low	âœ…
Decision Tree	âŒ No	High	âœ…
Random Forest	âŒ No	Low	âŒ</p>
              `,
            },
          ],
        },
        {
          title: "Section 5 Title",
          content: [
            {
              type: "text",
              content: `<h3>Confusion Matrix + áƒ›áƒ˜áƒ¡áƒ˜ áƒ™áƒáƒ›áƒáƒáƒœáƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜</h3>
              <p>Binary classification example</p>`,
            },
            {
              type: "code",
              content: `import numpy as np
from sklearn.metrics import confusion_matrix

# true labels and predictions
y_true = np.array([1, 0, 1, 1, 0, 0, 1, 0])
y_pred = np.array([1, 0, 0, 1, 0, 1, 1, 0])

cm = confusion_matrix(y_true, y_pred)
cm

# Confusion Matrix structure
# [[TN  FP]
# [FN  TP]]

# TN, FP, FN, TP = cm.ravel()
# TN, FP, FN, TP
`,
            },

            {
              type: "text",
              content: `<h3>áƒ¨áƒ”áƒ¤áƒáƒ¡áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒ”áƒ¢áƒ áƒ˜áƒ™áƒ”áƒ‘áƒ˜</h3>
              `,
            },

            {
              type: "code",
              content: `from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

accuracy, precision, recall, f1
`,
            },

            {
              type: "text",
              content: `<h3>Multiclass metrics example</h3>
              `,
            },
            {
              type: "code",
              content: `from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_test, y_pred, average="weighted")
recall = recall_score(y_test, y_pred, average="weighted")
f1 = f1_score(y_test, y_pred, average="weighted")

precision, recall, f1

# Use average="weighted" for multiclass / imbalance
`,
            },

            {
              type: "text",
              content: `<h3>Balanced vs Imbalanced datasets</h3>
              `,
            },

            {
              type: "code",
              content: `# 4.1 Balanced dataset example
y_true = [0, 0, 1, 1]
y_pred = [0, 0, 1, 1]

accuracy_score(y_true, y_pred)


# ğŸ“Œ Accuracy works well here

4.2 Imbalanced dataset example (IMPORTANT)
y_true = [0,0,0,0,0,0,0,1]
y_pred = [0,0,0,0,0,0,0,0]

accuracy_score(y_true, y_pred)


# ğŸ“Œ Accuracy â‰ˆ 0.87 âŒ
# ğŸ“Œ Model never finds class 1
              `,
            },

            {
              type: "text",
              content: `<p>balanced data -> accuracy,
                imbalanced data -> precision / recall,
                FP costly -> precision,
                FN costly -> recall
                Need balance -> F1-score</p>
              `,
            },

            {
              type: "code",
              content: `from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))
# ğŸ“Œ Prints all metrics at once
# ğŸ“Œ Accepted in most exams
              `,
            },
          ],
        },
        {
          title: "Section 6 Title",
          content: [
            {
              type: "text",
              content: `<h3>áƒ°áƒ˜áƒáƒ”áƒ áƒáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜ (Hyperparameters)</h3>
              <p>ğŸ“Œ Definition: parameters you set before training, not learned from data.
Example:

KNN â†’ n_neighbors

Random Forest â†’ n_estimators, max_depth

SVC â†’ C, kernel</p>`,
            },

            {
              type: "text",
              content: `<h3>GridSearchCV â€“ áƒ“áƒáƒœáƒ˜áƒ¨áƒœáƒ£áƒšáƒ”áƒ‘áƒ áƒ“áƒ áƒáƒ áƒ˜áƒœáƒªáƒ˜áƒáƒ˜</h3>
              <p>Automatically searches best hyperparameters

Uses Cross-Validation to avoid overfitting</p>`,
            },

            {
              type: "text",
              content: `<h3>Example: KNN + GridSearchCV + Cross-Validation</h3>
              `,
            },
            {
              type: "code",
              content: `from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load data
data = load_iris()
X = data.data
y = data.target

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Scale features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define model
knn = KNeighborsClassifier()

# Define hyperparameter grid
param_grid = {
    "n_neighbors": [3, 5, 7, 9],
    "weights": ["uniform", "distance"]
}

# GridSearchCV with 5-fold cross-validation
grid = GridSearchCV(
    estimator=knn,
    param_grid=param_grid,
    cv=5,
    scoring="accuracy"
)

# Fit to train data
grid.fit(X_train, y_train)

# Best parameters and best score
best_params = grid.best_params_
best_score = grid.best_score_

# Evaluate on test set
best_model = grid.best_estimator_
y_pred = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)

best_params, best_score, test_accuracy
`,
            },

            {
              type: "text",
              content: `<h3>Example with Random Forest</h3>
              `,
            },

            {
              type: "code",
              content: `from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=42)

param_grid = {
    "n_estimators": [50, 100, 200],
    "max_depth": [None, 5, 10]
}

grid = GridSearchCV(rf, param_grid, cv=5, scoring="accuracy")
grid.fit(X_train, y_train)

best_model = grid.best_estimator_
y_pred = best_model.predict(X_test)
accuracy_score(y_test, y_pred)

              `,
            },
            {
              type: "text",
              content: `<p>Always split train/test first, then apply GridSearchCV on train

GridSearchCV automates trial & error

Use cross-validation â†’ reduces overfitting

After finding best parameters â†’ test on held-out test set</p>
              `,
            },
          ],
        },
        {
          title: "Section 7 Title",
          content: [
            {
              type: "text",
              content: `<h3>Cross-Validation â€“ áƒáƒ áƒ¡áƒ˜ áƒ“áƒ áƒ“áƒáƒœáƒ˜áƒ¨áƒœáƒ£áƒšáƒ”áƒ‘áƒ</h3>
              <p>Definition:

A method to evaluate model performance more reliably

Splits data into multiple folds â†’ trains & tests multiple times

Reduces variance / overfitting compared to single train/test split

Why use it:

Single train/test can overestimate or underestimate performance

Cross-validation â†’ average of multiple splits â†’ more robust metric</p>`,
            },

            {
              type: "text",
              content: `<h3>K-Fold Cross-Validation â€“ áƒáƒ áƒ˜áƒœáƒªáƒ˜áƒáƒ˜</h3>
              <p>K-Fold steps:

Split dataset into K equal folds

For each fold:

Use fold as test, rest as train

Train model â†’ compute metric

Average metric across all folds â†’ final performance estimate

ğŸ“Œ Example: K=5 â†’ each row is tested once, trained 4 times</p>`,
            },
            {
              type: "text",
              content: `<h3>Simple example using sklearn</h3>`,
            },

            {
              type: "code",
              content: `from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
import numpy as np

# Load data
data = load_iris()
X = data.data
y = data.target

# Scale features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Define model
model = SVC(kernel="rbf", C=1.0)

# 5-Fold Cross-Validation
scores = cross_val_score(model, X, y, cv=5, scoring="accuracy")

# Average score
np.mean(scores), scores

# cross_val_score automatically:

# Splits data â†’ trains â†’ tests â†’ repeats K times

# Returns array of scores per fold
`,
            },
          ],
        },
        {
          title: "Section 8 Title",
          content: [
            {
              type: "text",
              content: `<h3>Overfitting-áƒ˜áƒ¡ áƒáƒ›áƒáƒªáƒœáƒáƒ‘áƒ˜áƒ¡ áƒœáƒ˜áƒ¨áƒœáƒ”áƒ‘áƒ˜</h3>
              <p>High train accuracy, low test accuracy

Model performs well on seen data, poorly on unseen

Very complex model for small/simple dataset</p>`,
            },

            {
              type: "text",
              content: `<h3>Exam-style Python Example</h3>
              `,
            },
            {
              type: "code",
              content: `import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Simple dataset
X = np.array([[i] for i in range(1, 11)])
y = np.array([0,0,0,0,1,1,1,1,1,1])

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Overfitting example: very deep tree
model = DecisionTreeClassifier(max_depth=None)  # no limit
model.fit(X_train, y_train)

y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)

# ğŸ“Œ Usually output:

# Train accuracy = 100%

# Test accuracy < 100% â†’ overfitting detected`,
            },

            {
              type: "text",
              content: `<h3>Underfitting example</h3>
              `,
            },

            {
              type: "code",
              content: `<h3># Very shallow tree â†’ underfitting
model = DecisionTreeClassifier(max_depth=1)
model.fit(X_train, y_train)

y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)
</h3>
              `,
            },
            {
              type: "text",
              content: `<h3>Bonus: Quick Overfit vs Underfit visualization/h3>
              `,
            },

            {
              type: "code",
              content: `
              import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

X = np.arange(0,10,0.5).reshape(-1,1)
y = np.sin(X).ravel() + 0.1*np.random.randn(len(X))

# Underfit: linear model
model = LinearRegression()
model.fit(X, y)
plt.plot(X, model.predict(X), label="Underfit")

# Overfit: high degree poly
poly = PolynomialFeatures(degree=15)
X_poly = poly.fit_transform(X)
model.fit(X_poly, y)
plt.plot(X, model.predict(X_poly), label="Overfit")

plt.scatter(X, y, color='black')
plt.legend()
plt.show()

              `,
            },
          ],
        },
        {
          title: "Section 9 Title",
          content: [
            {
              type: "text",
              content: `<h3>áƒ™áƒšáƒáƒ¡áƒ¢áƒ”áƒ áƒ˜áƒ–áƒáƒªáƒ˜áƒ (Clustering)</h3>
              <p>Definition: grouping data points without labels

Unsupervised learning â†’ model tries to find natural clusters

ğŸ“Œ Common use cases: customer segmentation, image compression, anomaly detection</p>`,
            },

            {
              type: "text",
              content: `<h3>K-Means áƒ™áƒšáƒáƒ¡áƒ¢áƒ”áƒ áƒ˜áƒ–áƒáƒªáƒ˜áƒ˜áƒ¡ áƒáƒšáƒ’áƒáƒ áƒ˜áƒ—áƒ›áƒ˜</h3>
              <p>Algorithm steps:

Choose K clusters

Initialize centroids randomly

Assign each point to the nearest centroid

Recalculate centroids based on assigned points

Repeat steps 3â€“4 until convergence</p>`,
            },
            {
              type: "code",
              content: `# example
              import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Example dataset: 2D points
X = np.array([
    [1,2],[1,4],[1,0],
    [4,2],[4,4],[4,0],
    [10,2],[10,4],[10,0]
])

# K-Means
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)

# Cluster labels
labels = kmeans.labels_
centroids = kmeans.cluster_centers_

print("Labels:", labels)
print("Centroids:", centroids)

# Plot clusters
plt.scatter(X[:,0], X[:,1], c=labels, cmap="viridis")
plt.scatter(centroids[:,0], centroids[:,1], color='red', marker='X', s=200)
plt.show()
`,
            },

            {
              type: "text",
              content: `<h3>Parameter K</h3>
              <p>K = number of clusters

Common methods to choose K:

Elbow method â†’ plot inertia_ vs K â†’ find â€œelbowâ€

Silhouette score â†’ higher = better separation</p>`,
            },
            {
              type: "code",
              content: `inertia = []
for k in range(1, 6):
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(X)
    inertia.append(km.inertia_)

plt.plot(range(1,6), inertia, marker='o')
plt.xlabel("K")
plt.ylabel("Inertia")
plt.show()
`,
            },

            {
              type: "text",
              content: `<h3>K-Means áƒ¨áƒ”áƒ–áƒ¦áƒ£áƒ“áƒ•áƒ”áƒ‘áƒ˜ (Limitations)</h3>
              <p>limitations: 
                Needs K in advance
Must guess number of clusters, Sensitive to initial centroids
Different results if random, Assumes spherical clusters
Struggles with weird shapes, Sensitive to outliers
Can distort centroids, Works best with scaled/normalized data
Otherwise distance metric fails</p>`,
            },
          ],
        },
        {
          title: "Section 9 Title",
          content: [
            {
              type: "text",
              content: `<h3>Dimensionality Reduction â€“ áƒáƒ áƒ¡áƒ˜</h3>
              <p>Definition: Reduce number of features â†’ simpler, faster, less noise

Useful when dataset has many correlated variables

Main goal â†’ retain most information with fewer features</p>`,
            },

            {
              type: "text",
              content: `<h3>PCA (Principal Component Analysis)</h3>
              <p>Concept:

Finds principal components (new axes) that capture maximum variance

Each PC = linear combination of original features

First PC â†’ explains most variance, second PC â†’ next most, etc.</p>`,
            },
            {
              type: "code",
              content: `import pandas as pd
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load data
data = load_iris()
X = data.data
y = data.target

# Scale features â†’ REQUIRED for PCA
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA transformation
pca = PCA(n_components=2)  # reduce to 2 dimensions
X_pca = pca.fit_transform(X_scaled)

# Explained variance
print("Explained variance ratio:", pca.explained_variance_ratio_)

# Plot
plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap='viridis')
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("PCA of Iris Dataset")
plt.show()

# Key points in code

# StandardScaler â†’ mandatory before PCA

# PCA(n_components=...) â†’ choose how many PCs to keep

# explained_variance_ratio_ â†’ tells how much info each PC keeps
`,
            },

            {
              type: "text",
              content: `<h3>PCA steps:</h3>
              <p>1. Scale features
2. Fit PCA(n_components)
3. Transform data
4. Check explained_variance_ratio_
5. Use reduced data for visualization or model</p>`,
            },
          ],
        },
      ];

      const mainContent = document.getElementById("mainContent");
      const navLinks = document.querySelectorAll(".nav-link");

      // Create all pages
      sections.forEach((section, index) => {
        const page = document.createElement("div");
        page.className = `page ${index === 0 ? "active" : ""}`;
        page.id = `page-${index}`;

        let pageHTML = `<h1 class="page-title">${section.title}</h1>`;

        section.content.forEach((item) => {
          if (item.type === "text") {
            pageHTML += `<div class="text-section">${item.content}</div>`;
          } else if (item.type === "code") {
            pageHTML += `<div class="code-block"><pre>${escapeHtml(item.content)}</pre></div>`;
          }
        });

        page.innerHTML = pageHTML;
        mainContent.appendChild(page);
      });

      // Navigation functionality
      navLinks.forEach((link) => {
        link.addEventListener("click", (e) => {
          e.preventDefault();
          const pageIndex = link.getAttribute("data-page");

          navLinks.forEach((l) => l.classList.remove("active"));
          document
            .querySelectorAll(".page")
            .forEach((p) => p.classList.remove("active"));

          link.classList.add("active");
          document.getElementById(`page-${pageIndex}`).classList.add("active");

          mainContent.scrollTop = 0;
        });
      });

      function escapeHtml(text) {
        const div = document.createElement("div");
        div.textContent = text;
        return div.innerHTML;
      }
    </script>
  </body>
</html>
